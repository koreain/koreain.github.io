---
title: "강화학습의 개요 01 (개념)"
excerpt: "강화학습 개념 공부"
toc: true
toc_stick: true
use_math: true
categories:
  - RL
tags:
  - 기계학습
  - 강화학습
---

# 강화학습의 개념
*강화학습 관련 모든 블로그 내용은 기본적으로 시중에 판매되는 "파이썬과 케라스로 배우는 강화학습 - 이웅원 외 4인"  저서를 기본으로 참고하며 필요 시, 개인적으로 검색한 레퍼런스나 자체 제작 컨텐츠를 삽입하는 형태로 작성합니다.*

## 1. 스키너의 강화연구
강화학습의 개념을 이해하려면 행동심리학과 강화학습 그리고 머신러닝과 강화학습의 연관성에 대해 알아야합니다. 행동심리학에서 "강화(Reinforcement)"라는 개념은 상당히 보편적으로 알려진 개념입니다. 강화는 동물이 "시행착오(Trial and Error)"를 통해 학습하는 방법 중 하나입니다. 사람도 마찬가지로 태어난 순간부터 살아가는 내내 수많은 상황에 노출되고, 선택과 행동을 통해 특정 상황에서 어떠한 행동을 해야 좋을지를 학습해 나갑니다.  이렇듯 강화라는 개념을 처음으로 제시한 것은 스키너(Skinner)라는 행동심리학자 입니다.  

행동심리학에는 시행착오 학습이라는 개념이 있습니다. 시행착오 학습은 동물들이 이것저것 시도해보면서 그 결과를 통해 학습하는 것을 말합니다. 스키너는 쥐 실험을 통해 동물이 행동과 그 결과 사이의 관계를 학습하는 것을 확인했습니다. 스키너는 상자 안에 굶긴 쥐를 집어넣고 아래 그림과 같이 실험을 수행했습니다.  

<center><img src="https://github.com/koreain/koreain.github.io/blob/master/_posts/content/images/skinner_mouse.jpg?raw=true" width="70%" height="70%" alt="Skinner's Mouse Experiment"></center>

스키너 쥐 실험의 프로세스는 아래와 같습니다.
1. 굶긴 쥐를 상자에 넣는다.
2. 쥐는 돌아다니다가 우연히 상자 안에 있는 지랫대를 누르게 된다.
3.  지렛대를 누르자 먹이가 나온다.
4.  지렛대를 누르는 행동과 먹이가 나오는 상관관계를 모르는 쥐는 다시 돌아다닌다.
5.  그러다가 우연히 쥐가 다시 지렛대를 누르면 쥐는 이제 지랫대를 누르는 행동이 먹이가 나오는 것과 관계가 있음을 알게 되고 점점 지렛대를 자주 누르게 된다.
6. 이 과정을 반복하면서 쥐는 지렛대를 누르면 먹이를 먹을 수 있다는 것을 학습한다.  

이처럼 강화라는 개념은 동물이 이전에 배우지 않았지만 직접 시도하면서 행동과 그 결과로 나타나는 보상 사이의 상관관계를 학습하는 것입니다. 그러면서 동물이 좋은 보상을 얻게 해주는 행동을 점점 더 많이 하는 개념을 말합니다.  

강화의 핵심은 쥐가 점점 보상을 얻게 해주는 행동을 자주 한다는 것입니다. 이때 쥐는 지렛대를눌렀을 때 왜 먹이가 나오는지 모르지만 지렛대를 누르면 먹이가 나온다는 건 알 수 있게 됩니다. 따라서 이해는 못하더라도 행동과 행동의 결과를 보상을 통해 연결할 수 있습니다.  

이는 아기가 처음 걷는 것을 배우는 과정과도 동일하게 연결하여 생각할 수 있습니다.  

## 2. 머신러닝과 강화학습
머신러닝은 인공지능의 한 범주로서 컴퓨터가 스스로 학습하게 하는 알고리즘을 개발하는 분야입니다. 1959년 아서 사무엘은 머신러닝을 "기계가 일일이 코드로 명시하지 않은 동작을 **데이터로부터 학습**해서 실행할 수 있도록 하는 알고리즘을 개발하는 연구 분야"라고 정의했습니다. 즉 미리 프로그램돼 있는 대로 작동하는 것이 아니고 주어진 데이터를 토대로 스스로 성능을 높여가는 것을 말합니다.  머신러닝은 크게 "지도학습", "비지도학습" 그리고 "강화학습" 3개의 부류로 나뉩니다.  (머신러닝에 대한 기본적인 내용은 [ML Introduction]( {{ site.url }}{{ site.baseurl }}/ml/ml-intro/)를 참고하세요 )

지도학습은 "정답"을 알고 있는 데이터를 이용해 컴퓨터를 학습시키고, 컴퓨터는 자신이 낸 답과 정담의 차이를 통해 지속해서 학습합니다. 회귀분석(Regression Analysis)과 분류(Classification) 등 많은 머신러닝 기법들이 이 학습에 해당합니다. 지도학습은 딥러닝과도 깊은 연관관계가 있습니다. 딥러닝의 예로 가장 유명한 MNIST 예제는 손글씨로 쓴 숫자를 보고 어떤 숫자인지 맞추는 것입니다.  

비지도학습은 지도학습과는 다르게 정답이 있는 것이 아닙니다. 그림에서처럼 그저 여러 개의 점이 주어지면 비슷한 것끼리 묶어주는 식의 학습이 비지도학습입니다. 이와 같이 비지도학습은 정답이 없이 주어진 데이터로만 학습합니다. 대표적으로 군집화(Clustering)가 비지도학습에 해당합니다. 실제로 군집화를 이용해 페이스북에서 비슷한 성향을 가진 사람들끼리 묶거나 어떤 제품에 대해 시장조사를 할 때 시장을 그 특성에 따라 나눌 수 있습니다.  

강화학습은 지도학습, 비지도학습과 그 성격이 다릅니다. 따라서 강화학습은 머신러닝에서 따로 분류됩니다. 정답이 주어진 것은 아니지만 그저 주어진 데이터에 대해 학습하는 것도 아니기 때문입니다. 강화학습은 "보상(Reward)"을 통해 학습합니다. 보상은 컴퓨터가 선택한 행동(Action)에 대한 환경의 반응입니다. 이 보상은 직접적인 답은 아니지만 컴퓨터에게는 간접적인 정답의 역할을 합니다.

지도학습에서는 직접적인 정답을 통해 오차를 계산해서 학습했지만 강화학습에서는 자신의 행동의 결과로 나타나는 보상을 통해 학습합니다. 강화학습을 수행하는 컴퓨터는 행동심리학에서 살펴본 "강화"처럼 보상을 얻게 하는 행동을 점점 더 많이 하도록 학습합니다.


## 3. 스스로 학습하는 컴퓨터, 에이전트
앞으로 강화학습을 통해 스스로 학습하는 컴퓨터를 에이전트(Agent)라고 할 것입니다. 에이전트는 환경에 대해 사전지식이 없는 상태에서 학습을 합니다. 에이전트는 자신이 놓은 환경에서 자신의 상태를 인식/관찰한 후 행동합니다. 그러면 환경은 에이전트에게 보상을 주고 다음 상태를 알려줍니다. 이 보상을 통해 에이전트는 어떤 행동이 좋은 행동인지 간접적으로 알게 됩니다. 이러한 보상을 지속해서 얻는다면 에이전트는 좋은 행동을 학습할 수 있습니다. 아래 그림은 이 과정을 그림으로 나타낸 것입니다.

![MDP](https://github.com/koreain/koreain.github.io/blob/master/_posts/content/images/MDP.png?raw=true "Markov Decision Process")

에이전트는 자신의 행동과 행동의 결과를 보상을 통해 학습하면서 어떤 행동을 해야 좋은 결과를 얻게 되는지 알게 됩니다. 따라서 에이전트는 점점 보상을 받는 행동을 자주 하게 되고 환경으로부터 더 많은 보상을 얻게 됩니다. 강화학습의 목적은 에이전트가 환경을 탐색하면서 얻는 보상들의 합을 최대화하는 "최적의 행동양식, 또는 정책"을 학습하는 것입니다.

보상은 양수일수도 음수일수도 있습니다. 보상을 음수로 설정한다면 처벌(패널티)가 됩니다. 상벌을 적절히 융합할 수 있다면 효과적인 학습이 가능합니다. 상만 받거나 벌만 받는 것보다 에이전트는 적절한 상벌을 통해 자신이 무엇을 해야 하는지 더 명확하게 알 수 있습니다. 이것은 강화학습의 중요한 문제로서 실질적으로 강화학습을 문제에 적용할 때 고려해야 하는 점입니다. 

이러한 강화학습의 장점은 환경에 대한 사전지식이 없어도 학습한다는 것입니다. 예를 들면 우리가 난생 처음 자전거를 타기를 배울 때와 유사한 학습방식입니다. 처음 자전거 타기를 배울 때 자전거의 무게나 탄성이 얼마고 이 정도 속도로 페달을 밟았을 때 RPM이 어느 정도인지를 미리 다 분석하고 관찰한 이후에 타보는 작업을 하지 않습니다. 대부분 자전거에 대해서는 아무것도 모르지만 자전거를 실제 타보면서 어떻게 자전거를 타는지 배웠을 것 입니다. 처음에는 휘청거리면서 넘어지기도 하지만 어떻게 해야 자전거가 넘어지지 않고 앞으로 잘 나갈 수 있는지 학습하면서 결국 자전거를 부드럽게 탔을 것입니다. 

이처럼 환경에 대한 정확한 지식이 없어도 학습할 수 있다는 점은 강화학습의 상당한 장점입니다. 실제 세상에서 에이전트가 어떠한 기능을 학습하려면 다양한 상황에 대한 정보가 있어야 합니다. 이러한 정보 없이 에이전트는 시행착오를 통해 어떠한 기능을 학습합니다. 또한 에이전트가 미리 아는 환경이라도 환경에 대한 정보를 통해 계산하려면 많은 시간이 걸립니다.

알파고(AlphaGo)에 대해 강화학습 관점에서 생각해보면 알파고 또한 바둑이라는 게임의 규칙과 사전지식이 없는 상태에서 바둑을 두면서 학습한 것입니다. 처음에는 무작위로 바둑돌을 놓다가 어쩌다가 상대방을 이기게 됩니다. 그러면 에이전트는 보상을 받고 상대방을 이기게 한 행동을 더 하려고 합니다.

이기는 숫자가 늘어갈수록 알파고는 어떤 상황에서 돌을 어디에 놔야 이기게 되는지를 학습합니다. 수많은 모의대국을 치르면서 바둑이 무엇인지도 모르는 알파고가 세계적인 수준의 프로기사와 바둑을 둘 정도로 성장하게 된 것입니다. 실제로 알파고는 바둑을 학습할 때 사람이 둔 기보를 통해 지독학습을 하는 단계도 있고, 학습된 에이전트끼리 모의대국을 통해 학습을 하는 단계도 있습니다.

## 4. 강화학습 문제
강화학습은 결정을 순차적으로 내려야 하는 문제에 적용됩니다. 아래 그림은 순차적 결정문제를 그림으로 나타낸 것으로 현재 위치에서 행동을 한번 선택하는 것이 아니라 계속적으로 선택해야 합니다. 

![Seq_Action_Decision](https://github.com/koreain/koreain.github.io/blob/master/_posts/content/images/Seq_Action_Decision.png?raw=true "Sequential Action Decision")

하지만 이렇게 순차적으로 결정을 내리는 문제의 해결책이 강화학습만 있는 것은 아닙니다. 향후에 다루게 될 다이내믹 프로그래밍(Dynamic Programming), 진화 알고리즘(Evolutionary Algorithm) 또한 이러한 문제를 푸는 데 적용할 수 있습니다. 하지만 두 방법론 모두 각기 한계를 가지고 있으며 강화학습이 그 한계를 극복할 수 있습니다.

## 5. 순차적 행동 결정 문제
에이전트가 학습하고 발전하려면 문제를 수학적으로 표현해야 합니다. 그렇지 않으면 에이전트의 입장에서는 학습을 하거나 최적화하기 어려울 것입니다. 순차적으로 행동을 결정하는 문제를 정의할 때 사용하는 방법이 MDP(Markov Decision Process) 입니다. MDP는 순차적 행동 결정 문제를 수학적으로 정의해서 에이전트가 순차적 행동 결정 문제에 접근할 수 있게 합니다.

## 6. 순차적 행동 결정 문제의 구성 요소
순차적 행동 결정 문제를 풀기 위해서는 문제를 수학적으로 정의해야 합니다. 수학적으로 정의된 문제는 다음과 같은 구성 요소를 가집니다. 이 구성 요소들을 MDP라고 부르며 이 후 포스트에서 자세히 다룰 것입니다.  

### 1. 상태(State)
에이전트의 상태로서 공학에서 많이 사용하는 개념입니다. 상태라고 하면 어떠한 정적인 요소만 포함한 현재 에이전트의 정보라고 보통 생각하시는데 그뿐만 아니라 에이전트가 움직이는 속도와 같은 동적인 요소 또한 상태로 표현할 수 있습니다. 상태의 정의가 중요한데, 에이전트가 상태를 통해 상황을 판단해서 행동을 결정하기에 충분한 정보를 제공해야 합니다. 엄밀히 말하면 상태보다는 "관찰"이라는 것이 정확한 표현입니다. 테니스를 치는 에이전트라고 생각해봅시다. 테니스공의 위치만 알고 속도를 모른다면 에이전트는 사실상 테니스를 칠 수가 없습니다. 에이전트가 테니스를 치는 것을 학습하려면 테니스공의 위치, 속도, 가속도 같은 정보가 필요합니다.

### 2. 행동(Action)
에이전트가 어떠한 상태에서 취할 수 있는 행동으로서 "상", "하", "좌", "우"와 같은 것을 말합니다. 게임에서의 행동이라면 게임기를 통해 줄 수 있는 입력일 것입니다. 학습이 되지 않은 에이전트는 어떤 행동이 좋은 행동인지에 대한 정보가 없으므로 처음에는 무작위 행동을 취하고 학습 과정을 거치면서 특정 상태에서 특정한 행동들을 할 확률을 높입니다. 에이전트가 행동을 취하면 환경은 에이전트에게 보상을 주고 다음 상태를 알려줍니다.

### 3. 보상(Reward)
보상은 강화학습을 다른 기계학습 기법과 다르게 만들어주는 가장 핵심적인 요소 입니다. 사실상 에인전트가 학습할 수 있는 유일한 정보가 바로 보상입니다. 이 보상이라는 정보를 통해 에이전트는 자신이 했던 행동들을 평가할 수 있고, 이로 인해 어떤 행동이 좋은 행동인지 알 수 있습니다.

앞에서 말했듯이 강화학습의 목표는 시간에 따라 얻는 보상들의 합을 최대로 하는 정책을 찾는 것입니다. 보상은 에이전트에 속하지 않는 환경의 일부 입니다. 에이전트는 어떤 상황에서 얼마의 보상이 나오는지 미리 알지 못합니다. 추정할 수 있을 뿐입니다.

### 4. 정책(Policy)
순차적 행동 결정 문제에서 구해야할 답은 바로 정책입니다. 에이전트가 보상을 얻으려면 행동을 해야 하는데  특정 상태가 아닌 모든 상태에 대해 어떤 행동을 해야 할 지 알아야 합니다. 이렇게 모든 상태에 대해 에이전트가 어떤 행동을 해야 하는지 정해놓은 것이 정책입니다.

순차적  행동 결정 문제를 풀었다고 한다면 제일 좋은 정책을 에이전트가 얻었다는 것입니다. 제일 좋은 정책은 최적 정책(Optimal Policy)라고 하며 에이전트는 최적 정책에 따라 행동했을 때 보상의 합을 최대로 받을 수 있습니다.

강화학습은 **문제의 정의를 어떻게 설정**하느냐에 따라 학습을 잘하는지가 결정됩니다. 따라서 **적절한 보상**을 받으며 에이전트가 학습할 수 있게 하는 것이 중요합니다. 그리고 에이전트가 **판단하기에 충분한 정보를 얻을 수 있도록 순차적 행동 결정 문제를 정의**해야 합니다.

## 7. 방대한 상태를 가진 문제에서의 강화학습
강화학습은 최근에 방대한 상태를 가진 문제에서 뛰어난 성능을 보여주고 있습니다. 그러한 점을 가장 잘 보여주는 것이 바로 알파고입니다. 바둑에서 가능한 경우의 수는 $10^{360}$ 으로서 우주의 원자 수인 $10^{80}$ 에 비해서도 월등히 많은 수입니다.

이것이 바로 1979년에 IBM의 딥블루(Deep Blue)가 세계 챔피언을 이긴 후에 오랫동안 컴퓨터가 바둑을 정복하지 못한 이유이고 딥마인드(DeepMind)가 바둑에 도전한 이유입니다. 따라서 단순히 체스를 이기고 좀 더 어려운 바둑을 이긴 것이 아니라 **상태가 아무리 많더라도 문제를 풀 수 있다는 것을 증명**한 것입니다.

하지만 이보다 더 많은 상태를 가지고 있는 문제가 바로 로봇의 학습 문제입니다. 로봇이 관찰하는 **정보와 행동, 보상이 모두 연속적**이기 때문에 사실상 **가능한 경우의 수는 무한대**라고 할 수 있습니다. 그리고 환경이 통제된 실험실이 아니라 실제 밖에서 로봇이 학습하려면 이미 문제는 단순히 계산으로 풀 수 있는 정도를 넘어 섭니다. 따라서 로봇에 강화학습을 적용하려면 알파고와 같이 수많은 상태에 대한 정보를 함수와 같은 형태로 근사하는 인공신경망(Artificial Neural Network)을 사용해야 합니다. 강화학습과 인공신경망의 조합은 알파고에서만 빛을 발하는 것이 아닙니다. 강화학습과 인공신경망의 조합은 현실 세계의 문제를 학습할 수 있는 길을 열어주고 있습니다. 그렇다고 해서 당장 강화학습이 로봇이나 에이전트가 인간처럼 지능적으로 판단하고 행동할 수 있게 해주는 것은 아닙니다. 이제야 간단한 기능들을 학습하기 시작했기 때문입니다. 이러한 연구들이 쌓여서 그 간단한 기능을 가지고 어떠한 작업을 하고 그러한 작업을 토대로 어떠한 임무를 수행할 수 있을 때 지능이라는 말에 좀 더 적합할 것입니다.

## 8. 강화학습의 예시: 브레이크 아웃
아타리(Atari)의 고전 게임인 브레이크아웃(Breakout)에 강화학습을 적용하려면 어떻게 할까요? 아타리 게임에 강화학습을 적용한 논문이 딥마인드의 "[Playing Atari with Deep Reinforcement Learning](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)" 입니다. 이 논문에서는 다양한 아타리 게임에 강화학습을 적용했는데, 그 중에서도 가장 유명한 게임 중 하나인 브레이크아웃을 예시로 살펴보겠습니다.

딥마인드에서는 이 게임을 학습하는 동영상을 유튜브에 공개했습니다. 이 동영상을 보면 에이전트는 단순히 벽돌만 깨는 것을 학습한 것이 아니고 한 쪽을 터널로 뚫어서 여러 개의 벽돌을 한꺼번에 깨버리는 것도 학습했습니다. 아래 영상을 통해 에이전트가 어떻게 브레이크아웃을 학습시키는지 간략하게 확인해 보실 수 있습니다. 영상 1:25 부분에서 보면 한쪽 구멍을 뚫어서 위쪽 벽돌을 한꺼번에 깨버리는 전략을 학습하기도 합니다.

<center><a href="https://www.youtube.com/watch?v=V1eYniJ0Rnk?t=0s"><img src="https://img.youtube.com/vi/V1eYniJ0Rnk/0.jpg" alt="Google DeepMinds's Deep Q-learning"></a></center>
